<!doctype html><html prefix="og: https://ogp.me/ns#" lang=en><head><meta charset=utf-8><link rel=canonical href=https://uthark.github.io/post/2021-01-13-service-cidr-block-migration/><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,minimum-scale=1,maximum-scale=5"><title>Performing Kubernetes Service CIDR Block Migration</title><meta name=description content><meta name=keywords content><meta name=author content="Oleg Atamanenko"><link rel=stylesheet media=screen href="https://fonts.googleapis.com/css2?family=Lora:wght@500&family=Source+Code+Pro:wght@300&family=Bitter&display=swap" type=text/css><link rel=stylesheet href=https://uthark.github.io/css/theme.e2ab29fcebb06aaf6e0d5f5685d4c3f28ae29e6bdab99de41ced527d1770a0f6.css><link rel=stylesheet href=https://uthark.github.io/css/uthark.css><link rel=stylesheet href=//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css><link href=https://uthark.github.io/favicon.png rel=icon><link href=https://feeds.feedburner.com/atamanenko rel=alternate type=application/rss+xml title="Sharing knowledge"><meta name=twitter:card content="summary"><meta name=twitter:site content="@real_atamanenko"><meta name=twitter:creator content="@real_atamanenko"><meta name=twitter:title content="Performing Kubernetes Service CIDR Block Migration"><meta property="og:title" content="Performing Kubernetes Service CIDR Block Migration"><meta property="og:url" content="https://uthark.github.io/post/2021-01-13-service-cidr-block-migration/"><meta property="og:description" content="In the post I describe how we did migration of Service CIDR Blocks in the fleet of our kubernetes clusters in production without downtime."></head><body><header class=site><h1><a href=https://uthark.github.io/>Sharing knowledge</a></h1><h2>by Oleg Atamanenko</h2></header><nav><div class=menu><input type=checkbox id=menu class="fa fa-bars">
<label for=menu aria-label=Menu></label><div class=menu-container><ul class=main-navigation><li><a href=https://uthark.github.io/ title=Posts>Posts</a></li><li><a href=https://uthark.github.io/usefularticles/ title=Links>Links</a></li><li><a href=https://uthark.github.io/categories/ title=Categories>Categories</a></li><li><a href=https://uthark.github.io/readinglist/ title="Reading Lists">Reading Lists</a></li><li><a href=https://uthark.github.io/about/ title=About>About</a></li></ul><ul class=subscription><li><a href=https://feeds.feedburner.com/atamanenko target=_blank type=application/rss+xml title=RSS><i class="fa fa-rss-square fa-lg"></i></a></li></ul><form action=https://www.google.com/search method=get target=_blank><fieldset role=search><input class=search type=text name=q placeholder=Search>
<input type=hidden name=q value=site:https://uthark.github.io/></fieldset></form></div></div></nav><div id=main><div id=content><div><article class=hentry><header><h1 class=entry-title>Performing Kubernetes Service CIDR Block Migration</h1></header><div class=entry-content><p>In the post I describe how we did migration of Service CIDR Blocks in the fleet of our kubernetes clusters in production without downtime.</p><p>Note: This is a copy of my blog post published in <a href=https://medium.com/zendesk-engineering/performing-kubernetes-service-cidr-block-migration-8f554cb4d4f>Zendesk Engineering on medium</a></p><h1 id=history>History</h1><p>At Zendesk, we started our Kubernetes journey in September 2015, shortly after version 1.0 was publicly available. At the time of v1.0 not much information was available and we didn’t have a lot of experience, so some configuration choices that made sense four years ago have had to be reconsidered as the years went by.</p><p>Our clusters have seen multiple potentially disruptive migrations in production. For example in 2018, we changed the CNI implementation from flannel to AWS VPC CNI Plugin, in 2019 we migrated our etcd installation from v2 to v3 data format, and most recently we performed a Service CIDR Block migration across our live production clusters.</p><p>When we originally provisioned clusters we assumed having ClusterIPs in different clusters allocated from the same CIDR Range block would be fine since Service ClusterIPs are only resolvable in-cluster, and therefore we would be conserving valuable private IPv4 network space. So, each of our clusters was configured using the same RFC 1918 private CIDR block for service ClusterIPs.</p><p>Since that time hyper-growth in both the scale of our infrastructure and adoption of our Kubernetes platform at Zendesk has led us to consider spreading related microservices across multiple clusters, connected by service mesh technology. Clearly, having the same ClusterIP assigned to different services across clusters had downsides for service discovery and routing that needed to be fixed.</p><h1 id=research>Research</h1><p>Testing quickly showed that changing the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/><code>--service-cluster-ip-range flag</code></a> on the API Server would not be enough. Existing services would continue to use the old ClusterIPs, whereas kube-proxy would no longer populate IPTables rules for these services, whose old ClusterIPs would then be outside of the configured range. Beyond this, we found that ClusterIPs are immutable. In order to update them each service would need to be deleted and recreated to pick up it’s new IP, thus creating a short but unacceptable service interruption in production.
To overcome these issues, we performed a few rounds of research and tried different approaches for migration.
We had a brainstorming session and identified several options that might work out:</p><ol><li><strong>Copy the service</strong> — our first idea was to create a copy of the service and set externalIPs of the service to point to the original Cluster IP. This didn’t work out because kube-proxy ignores external IPs, assuming the underlying network will route to them directly.</li><li><strong>Migration</strong> — using deletion of the old service and creating a new one. The idea was to set service-cluster-ip-range flag to the new CIDR block, delete the old service, create a copy of the old service using old cluster IP, and recreate the service with a new cluster IP. This didn’t work because API Server rejects to create service with ClusterIP outside of the service IP range. Moreover, after deleting the service the change is propagated everywhere and we will have small connectivity blips. Decided not to go this route.</li><li><strong>“Stale” TTL support</strong> — our next idea was to try updating kube-proxy to add support for long lived service TTLs. This way, when a service is recreated, kube-proxy still would have to remember the old IP address and continue to route to it for some time. We decided against this option because the “stale” TTL could bring unintended results, i.e. if the migration were to be interrupted or delayed, then even our long TTL could expire, causing a service outage.</li><li><strong>Patch kube-proxy</strong> — our final decision was to add a temporary patch to kube-proxy to support routing external IPs as if they are ClusterIPs. Extensive testing showed us that this solution would be the only solution that would ensure no service outage in production.</li></ol><p>Finally, we outlined the following migration procedure:</p><ol><li>Patch kube-proxy and add support for two service CIDR blocks — one will be covering the old service CIDR block and another one would be used for the new Service CIDR Block. Also, treat External IPs as ClusterIPs if they are part of any of known service CIDR blocks. In our setup we don’t use external IPs, and we didn’t want to extend v1.Service with new fields. Obviously, this patch was specific to our setup and couldn’t be landed upstream, so we forked <a href=https://github.com/kubernetes/kubernetes/>k8s.io/kubernetes</a> and made our <a href=https://github.com/uthark/kubernetes/commit/b82dc8761084400ee86acf4076c5d63a3de80e75>small change to kube-proxy</a>. With this change, kube-proxy could generate IP Tables rules for multiple service CIDR blocks. So, after the change of the service-cidr-range flag on API Server, services that weren’t migrated yet still would be routable and continue to serve traffic.</li><li>A related patch was needed in Kubernetes API Server logic — <a href=https://github.com/uthark/kubernetes/commit/1152f6cd03fd0f18ea4aa745a9855c989c272022>allow a user to update an existing Service with a different ClusterIP</a>. This would allow us to update all services and assign them cluster IPs from the new Service CIDR block. This change would be disruptive without our patched version of kube-proxy</li><li>To perform the actual migration we developed a migration utility that would automate most of the steps and allow us to run pre-migration checks, cleanup and so on.</li></ol><h1 id=testing>Testing</h1><p>With that in place, we started to test the migration in our staging infrastructure and found a few other issues:</p><h2 id=serviceip-rangeallocation-object>ServiceIP RangeAllocation object</h2><p>During testing of the migration in our staging clusters, the migration would sometimes fail with an error indicating there are no free Cluster IPs available. After some digging in the kubernetes codebase, we found that there is a RangeAllocation object that stores a bitmap of used IP Addresses in the block, and if Service CIDR changes, the data becomes invalid and causes issues when new ClusterIP is allocated. So, we updated the migration procedure to stop the kube-controller-manager and remove the RangeAllocation object from etcd after confirming it wouldn’t cause any other issues.</p><h2 id=certificates-for-kubernetes-api-server>Certificates for Kubernetes API Server</h2><p>When we changed the service CIDR range, we needed to reissue certs used by Kubernetes API Servers. The new certs would need to include the ClusterIP address of the <code>kubernetes.default.svc.cluster.local</code> from the new Service CIDR range. However, to generate those certs we would need to know ahead of time which IP from the new CIDR block would be assigned to the Kubernetes service. So, our migration utility ensured that Kube API Server would get the first IP address from the configured Service CIDR range and we update code that requests certificates to follow the same logic.</p><h1 id=performing-the-migration>Performing the Migration</h1><p>After hosting pre-mortem meetings and performing the migration back and forth several times in staging we were confident that we could proceed to production. We deemed this change to be high risk in its nature, so we need to take extra precautionary steps. We worked with our Incident Management team and requested a maintenance window to perform the migration. To further reduce potential impact, we integrated the migration utility with an internal system to look up the criticality tier of the services, so that we could perform the migration for the less critical services first, before moving on to increasingly visible services.
From a technical perspective, the migration steps were the following:</p><ol><li>Rollout patched version of kube-proxy to all non-control plane nodes.</li><li>Lock deployment pipeline tools to prevent noise during migration.</li><li>Rollout patched API Servers.</li><li>Stop kube-controller-manager.</li><li>Delete Services RangeAllocation object from etcd.</li><li>Restart kube-controller-manager.</li><li>Perform migration in batches, per criticality tier.</li><li>Unlock deployment pipeline.</li><li>After migration is completed do a cleanup rollout.</li><li>Rollout all non-control plane nodes with a vanilla version of kube-proxy.</li><li>Rollout vanilla version of API Servers.</li><li>Perform migration cleanup and clean external IPs from old service cidr block from services.</li></ol><p>With all preparation done, we sent out our notification to customers, adjusted our timeframes a bit to meet their needs, and blocked a few weekends to perform the rollout in production. Thanks to detailed planning and preparation, we were able to complete our migration across all production clusters without even the smallest blip in QoS for Zendesk customers. This paved the way for cross-cluster service mesh, extending Kubernetes Service IP lookups and routing across cluster boundaries at Zendesk.</p></div><footer><div class=sharing>Share:
<a href="https://getpocket.com/save?url=https%3a%2f%2futhark.github.io%2fpost%2f2021-01-13-service-cidr-block-migration%2f" target=_blank><i class="fa fa-get-pocket" title="Save to Pocket" aria-hidden=true></i></a><a href="https://twitter.com/intent/tweet?text=Performing%20Kubernetes%20Service%20CIDR%20Block%20Migration&url=https%3a%2f%2futhark.github.io%2fpost%2f2021-01-13-service-cidr-block-migration%2f" target=_blank title="Post to Twitter" aria-hidden=true><i class="fa fa-twitter"></i></a></div><div><hr><p>If you have any questions, feel free to ping me on Twitter.</p></div><p class=meta>Estimated reading time: 7 minute(s)</p><p class=meta>Last update: <time datetime=2021-01-13>Jan 13, 2021</time></p><p class=meta>Categories: <a class=label href=https://uthark.github.io/categories/kubernetes/>#kubernetes</a></p></p><p class=meta><a class="basic-alignment left" href=https://uthark.github.io/post/2021-01-10-ios-shortcuts/ title="iOS shortcuts">&lt;&lt;&nbsp;iOS shortcuts</a>
<a class="basic-alignment right" href=https://uthark.github.io/post/2021-10-06-running-pihole-kubernetes/ title="Running Pi-hole on Kubernetes">Running Pi-hole on Kubernetes&nbsp;>></a></p><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"uthark"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></footer></article></div><aside class="sidebar thirds"><section class="first odd"><p></p></section><section class=even><div class=toc-header>Table of Contents</div><nav id=TableOfContents><ul><li><a href=#serviceip-rangeallocation-object>ServiceIP RangeAllocation object</a></li><li><a href=#certificates-for-kubernetes-api-server>Certificates for Kubernetes API Server</a></li></ul></nav></section></aside></div></div><footer><div class=center><ul class=sidebar-nav><li class=sidebar-nav-item><a target=_blank href=https://github.com/uthark/ title=https://github.com/uthark/><i class="fa fa-github fa-2x"></i></a></li><li class=sidebar-nav-item><a target=_blank href=https://bitbucket.org/uthark/ title=https://bitbucket.org/uthark/><i class="fa fa-bitbucket fa-2x"></i></a><li class=sidebar-nav-item><a target=_blank href=https://twitter.com/real_atamanenko title=https://twitter.com/real_atamanenko><i class="fa fa-twitter fa-2x"></i></a><li class=sidebar-nav-item><a target=_blank href=https://keybase.io/uthark/ title=https://keybase.io/uthark/><i class="fa fa-key fa-2x"></i></a><li class=sidebar-nav-item><a target=_blank href=http://stackoverflow.com/users/216764/uthark title=http://stackoverflow.com/users/216764/uthark><i class="fa fa-stack-overflow fa-2x"></i></a></li><li><a href=https://feeds.feedburner.com/atamanenko target=_blank type=application/rss+xml title=RSS><i class="fa fa-rss-square fa-2x"></i></a></li></ul></div><p>&copy;&nbsp;2009&nbsp;—&nbsp;2021 Oleg Atamanenko - <a href=https://uthark.github.io/license/>CC&nbsp;BY-SA&nbsp;4.0</a></p></footer><script>var _gaq=[['_setAccount','UA-8688665-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];s.async='async'
g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'));</script></body></html>